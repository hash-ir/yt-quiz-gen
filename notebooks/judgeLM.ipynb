{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "sys.path.append(os.path.dirname(SCRIPT_DIR))\n",
    "\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "from quiz.topics import extract_topics\n",
    "from quiz.generator import generate_quiz\n",
    "from quiz.judge import JUDGE_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "llm = Groq(model=\"llama-3.1-8b-instant\", api_key=GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched transcript with 22735 characters, approx 5683 tokens.\n"
     ]
    }
   ],
   "source": [
    "topics = extract_topics(llm, url=\"https://www.youtube.com/watch?v=2TJxpyO3ei4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Introduction to RAG Application': 'The video introduces a Python RAG (Retrieval Augmented Generation) application that allows users to ask questions about a set of PDFs using natural language. The application will provide an answer and a reference to the source material. The creator mentions that this is an advanced tutorial, building upon a previous basic RAG tutorial, and will cover features such as getting the application running locally using open-source LLMs, updating the vector database with new entries, and testing and evaluating the quality of AI-generated responses.',\n",
       " 'Overview of RAG and LLMs': 'The creator provides a quick recap of RAG and LLMs (Large Language Models). RAG is a way to index a data source so that it can be combined with an LLM, giving an AI chat experience that can leverage the data. The creator also mentions that LLMs are used to generate responses based on the data found in the PDF sources.',\n",
       " 'Loading and Preprocessing Data': \"The creator explains how to load PDF documents using the Langchain library and its built-in PDF document loader. The documents are then split into smaller chunks using Langchain's recursive text splitter. The creator also mentions the importance of using the same embedding function when creating the database and querying it.\",\n",
       " 'Creating a Vector Database': 'The creator explains how to create a vector database using ChromaDB. The database is populated with chunks of text from the PDF documents, each chunk having a unique but deterministic ID. The creator also mentions that ChromaDB allows for updating existing items in the database.',\n",
       " 'Implementing Local LLMs': 'The creator explains how to use a local LLM model, specifically Ollama, to generate responses. The creator mentions that Ollama is a platform that manages and runs open-source LLMs locally on the computer. The creator also mentions that using a local LLM model can be beneficial for generating responses, but may not always produce the best results.',\n",
       " 'Testing and Evaluating the Application': 'The creator explains how to test and evaluate the quality of the AI-generated responses using unit testing. The creator mentions that the application can be tested by providing sample questions and expected answers, and then comparing the actual responses generated by the application. The creator also mentions that the evaluation can be subjective and may not always produce accurate results.',\n",
       " 'Writing Unit Tests for LLM Applications': \"The creator provides an example of how to write unit tests for LLM applications using a helper function that returns true or false based on the evaluation of the LLM. The creator mentions that it's essential to have both positive and negative test cases and to set a threshold for what is considered a good enough result.\",\n",
       " 'Conclusion and Next Steps': 'The creator concludes the video by summarizing the main topics covered and encouraging viewers to explore further topics, such as deploying the application to the cloud. The creator also provides a link to the GitHub repository containing the code used in the video.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Overview of RAG and LLMs'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = random.choice(list(topics.keys()))\n",
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The creator provides a quick recap of RAG and LLMs (Large Language Models). RAG is a way to index a data source so that it can be combined with an LLM, giving an AI chat experience that can leverage the data. The creator also mentions that LLMs are used to generate responses based on the data found in the PDF sources.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc = topics[topic]\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What is RAG in the context of the YouTube video?',\n",
       "  ['a) A type of Large Language Model',\n",
       "   'b) A way to index a data source',\n",
       "   'c) A method for generating responses',\n",
       "   'd) A type of AI chat experience'],\n",
       "  'b) A way to index a data source',\n",
       "  'RAG is a way to index a data source so that it can be combined with an LLM, giving an AI chat experience that can leverage the data.'),\n",
       " ('What is the primary function of Large Language Models (LLMs) in the context of the video?',\n",
       "  ['a) To index data sources',\n",
       "   'b) To generate responses based on data',\n",
       "   'c) To provide AI chat experiences',\n",
       "   'd) To analyze data'],\n",
       "  'b) To generate responses based on data',\n",
       "  'LLMs are used to generate responses based on the data found in the PDF sources.'),\n",
       " ('What type of sources are used by LLMs in the context of the video?',\n",
       "  ['a) CSV files', 'b) PDF files', 'c) Excel files', 'd) Word documents'],\n",
       "  'b) PDF files',\n",
       "  'LLMs are used to generate responses based on the data found in the PDF sources.'),\n",
       " ('What is the result of combining RAG with an LLM in the context of the video?',\n",
       "  ['a) A more efficient data analysis process',\n",
       "   'b) A more accurate response generation process',\n",
       "   'c) An AI chat experience that can leverage the data',\n",
       "   'd) A more complex data indexing process'],\n",
       "  'c) An AI chat experience that can leverage the data',\n",
       "  'RAG is a way to index a data source so that it can be combined with an LLM, giving an AI chat experience that can leverage the data.'),\n",
       " ('What is the primary benefit of using RAG with LLMs in the context of the video?',\n",
       "  ['a) Improved data analysis',\n",
       "   'b) Enhanced response generation',\n",
       "   'c) Increased data indexing efficiency',\n",
       "   'd) More accurate AI chat experiences'],\n",
       "  'd) More accurate AI chat experiences',\n",
       "  'RAG is a way to index a data source so that it can be combined with an LLM, giving an AI chat experience that can leverage the data.')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz = generate_quiz(llm, topic, desc)\n",
    "quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the primary function of Large Language Models (LLMs) in the context of the video?',\n",
       " ['a) To index data sources',\n",
       "  'b) To generate responses based on data',\n",
       "  'c) To provide AI chat experiences',\n",
       "  'd) To analyze data'],\n",
       " 'b) To generate responses based on data')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question, choices, answer, expl = random.choice(quiz)\n",
    "question, choices, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = JUDGE_PROMPT.format(topic=desc, question=question, choices=\"\\n\".join(choices), answer=answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Feedback:::\\n'\n",
      " 'Rating: Medium\\n'\n",
      " '\\n'\n",
      " \"The question is rated as 'Medium' because:\\n\"\n",
      " '\\n'\n",
      " '- The sentence complexity of the question is relatively simple, with a clear '\n",
      " 'and direct question about the primary function of LLMs.\\n'\n",
      " \"- The vocabulary difficulty is moderate, with terms like 'Large Language \"\n",
      " \"Models' and 'generate responses', but they are not overly complex or \"\n",
      " 'technical.\\n'\n",
      " '- The concepts involved to answer the question are relatively '\n",
      " 'straightforward, as the question is based on a basic understanding of LLMs '\n",
      " 'and their role in generating responses based on data. However, it does '\n",
      " 'require some basic knowledge of the topic, which might make it slightly '\n",
      " 'challenging for viewers who are new to the subject.')\n"
     ]
    }
   ],
   "source": [
    "pprint(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvidia-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
