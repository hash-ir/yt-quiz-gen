{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hashir\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Hashir\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Hashir\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_kwargs\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Hashir\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPI has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Hashir\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from pprint import pprint\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index.core.program import LLMTextCompletionProgram\n",
    "from pydantic import BaseModel, parse_obj_as\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import disk_offload\n",
    "\n",
    "from data.transcript import get_video_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.environ['GROQ_API_KEY']\n",
    "NVIDIA_API_KEY = os.environ['NVIDIA_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = NVIDIA(model=\"meta/llama-3.1-8b-instruct\", api_key=NVIDIA_API_KEY)\n",
    "llm = Groq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = get_video_transcript(url=\"https://www.youtube.com/watch?v=zduSFxRajkE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for item in transcript:\n",
    "    text += item['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hi everyone so in this video I'd like usto cover the process of tokenization inlarge language models now you see herethat I have a set face and that'sbecause uh tokenization is my leastfavorite part of working with largelanguage models but unfortunately it isnecessary to understand in some detailbecause it it is fairly hairy gnarly andthere's a lot of hidden foot guns to beaware of and a lot of oddness with largelanguage models typically traces back totokenization so what istokenization now in m\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topic(BaseModel):\n",
    "    name: str\n",
    "    description: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/Llama3-ChatQA-2-70B\")\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tokenizer.encode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sllm = llm.as_structured_llm(output_cls=Topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_extractor_prompt = f\"\"\"\n",
    "Given the transcript of a YouTube video tutorial:\n",
    "\n",
    "{text[:-100000]}\n",
    "\n",
    "Understand what this video is about and identify the main topics.\n",
    "Include as much description as possible for each topic.\n",
    "Make sure topics are not ambiguous and have meaningful segregation of content.\n",
    "\n",
    "Format your response as a JSON array of objects, where each object has two fields:\n",
    "- \"name\": The name of the topic (a short, concise title)\n",
    "- \"description\": A detailed description of the topic\n",
    "\n",
    "Example format:\n",
    "[\n",
    "  {{\n",
    "    \"name\": \"Topic 1\",\n",
    "    \"description\": \"Detailed description of Topic 1...\"\n",
    "  }},\n",
    "  {{\n",
    "    \"name\": \"Topic 2\",\n",
    "    \"description\": \"Detailed description of Topic 2...\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "Remove the preamble and ensure your output is valid JSON. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(topic_extractor_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Introduction to Tokenization',\n",
       "  'description': \"The host introduces the topic of tokenization in large language models, explaining that it's a crucial but complex process. They mention that tokenization is their least favorite part of working with large language models, but it's necessary to understand in detail. The host also mentions that many issues with large language models can be traced back to tokenization.\"},\n",
       " {'name': 'Naive Tokenization',\n",
       "  'description': \"The host explains that in their previous video, they implemented a naive and simple version of tokenization. They used a vocabulary of 65 possible characters and created a lookup table to convert characters into tokens. The host shows an example of how this process works and how it's used to plug text into large language models.\"},\n",
       " {'name': 'Character-Level Tokenization vs. Chunk-Level Tokenization',\n",
       "  'description': 'The host explains that in state-of-the-art language models, people use more complicated schemes for constructing token vocabularies. They mention that character-level tokenization is used in naive tokenization, but chunk-level tokenization is used in more advanced models. The host mentions that the BPE (Byte Pair Encoding) algorithm is used to construct chunk-level token vocabularies.'},\n",
       " {'name': 'BPE (Byte Pair Encoding) Algorithm',\n",
       "  'description': 'The host explains that the BPE algorithm is used to compress byte sequences into a variable amount of tokens. They show an example of how the algorithm works, where it iteratively finds the pair of tokens that occur most frequently and replaces them with a new token. The host explains that this process can be repeated to compress the sequence further.'},\n",
       " {'name': 'UTF-8 Encoding',\n",
       "  'description': 'The host explains that UTF-8 is a variable-length encoding that takes every single Unicode code point and translates it into a byte stream. They mention that UTF-8 is the most common encoding and is backwards compatible with the ASCII encoding. The host shows an example of how to encode a string using UTF-8 and how to access the raw bytes of the encoding.'},\n",
       " {'name': 'UTF-8 Encoding Limitations',\n",
       "  'description': 'The host explains that using UTF-8 encoding naively would imply a vocabulary length of only 256 possible tokens, which is too small for most applications. They mention that this would result in very long sequences of bytes, which would be inefficient and not allow for sufficient context length in the Transformer model.'},\n",
       " {'name': 'Tokenization and Vocabulary Size',\n",
       "  'description': 'The host explains that the vocabulary size is a critical parameter in tokenization. They mention that a small vocabulary size would result in very long sequences of bytes, while a large vocabulary size would allow for more efficient tokenization. The host explains that the BPE algorithm can be used to compress byte sequences into a variable amount of tokens, allowing for a larger vocabulary size.'},\n",
       " {'name': 'Tokenization and Language Models',\n",
       "  'description': 'The host explains that tokenization is a critical component of language models. They mention that many issues with language models can be traced back to tokenization, such as difficulties with spelling tasks, non-English languages, and simple arithmetic. The host explains that the design of the tokenizer can have a significant impact on the performance of the language model.'},\n",
       " {'name': 'GPT-4 Tokenizer vs. GPT-2 Tokenizer',\n",
       "  'description': 'The host explains that the GPT-4 tokenizer has improved handling of whitespace for Python, grouping multiple spaces into a single token. They mention that this results in a more efficient representation of Python code and allows for better performance on tasks that involve code. The host shows an example of how the GPT-4 tokenizer handles whitespace compared to the GPT-2 tokenizer.'},\n",
       " {'name': 'Code Implementation',\n",
       "  'description': 'The host explains that they will now start writing code to implement tokenization. They mention that they want to take strings and feed them into language models, which requires tokenizing strings into integers in a fixed vocabulary. The host explains that they will use the BPE algorithm to compress byte sequences into a variable amount of tokens.'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hashir\\AppData\\Local\\Temp\\ipykernel_14300\\482209594.py:3: PydanticDeprecatedSince20: `parse_obj_as` is deprecated. Use `pydantic.TypeAdapter.validate_python` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  topics = parse_obj_as(List[Topic], json_response)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    json_response = json.loads(response.text)\n",
    "    topics = parse_obj_as(List[Topic], json_response)\n",
    "    topics_dict = {topic.name: topic.description for topic in topics}\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Error: The LLM output was not valid JSON.\")\n",
    "except ValueError:\n",
    "    print(\"Error: The LLM output did not match the expected format.\")\n",
    "    \n",
    "topics = ', '.join(list(topics_dict.keys()))\n",
    "topics_out = f\"\"\"I have identified the following topics from the video: {topics}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have identified the following topics from the video: Introduction to Tokenization, Naive Tokenization, Character-Level Tokenization vs. Chunk-Level Tokenization, BPE (Byte Pair Encoding) Algorithm, UTF-8 Encoding, UTF-8 Encoding Limitations, Tokenization and Vocabulary Size, Tokenization and Language Models, GPT-4 Tokenizer vs. GPT-2 Tokenizer, Code Implementation'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"\"\"[\n",
    "    {\n",
    "        \"question\": \"What is the primary goal of the BPE (Byte Pair Encoding) algorithm?\",\n",
    "        \"options\": [\n",
    "            \"a) To encrypt byte sequences\",\n",
    "            \"b) To compress byte sequences into a variable amount of tokens\",\n",
    "            \"c) To decrypt byte sequences\",\n",
    "            \"d) To convert byte sequences into a fixed amount of tokens\"\n",
    "        ],\n",
    "        \"answer\": \"b) To compress byte sequences into a variable amount of tokens\",\n",
    "        \"explanation\": \"The BPE algorithm is used to compress byte sequences into a variable amount of tokens, allowing for more efficient storage and processing.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does the BPE algorithm find the pair of tokens to replace?\",\n",
    "        \"options\": [\n",
    "            \"a) By randomly selecting pairs\",\n",
    "            \"b) By iterating through the sequence and finding the pair that occurs most frequently\",\n",
    "            \"c) By using a predefined set of rules\",\n",
    "            \"d) By using a machine learning model\"\n",
    "        ],\n",
    "        \"answer\": \"b) By iterating through the sequence and finding the pair that occurs most frequently\",\n",
    "        \"explanation\": \"The BPE algorithm iteratively finds the pair of tokens that occur most frequently in the sequence and replaces them with a new token.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What happens when the BPE process is repeated?\",\n",
    "        \"options\": [\n",
    "            \"a) The vocabulary size decreases\",\n",
    "            \"b) The vocabulary size remains the same\",\n",
    "            \"c) The vocabulary size increases\",\n",
    "            \"d) The sequence becomes less compressed\"\n",
    "        ],\n",
    "        \"answer\": \"c) The vocabulary size increases\",\n",
    "        \"explanation\": \"When the BPE process is repeated, the vocabulary size increases as more pairs of tokens are replaced with new tokens.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the result of replacing a pair of tokens with a new token in the BPE algorithm?\",\n",
    "        \"options\": [\n",
    "            \"a) The sequence becomes less compressed\",\n",
    "            \"b) The sequence remains the same\",\n",
    "            \"c) The sequence becomes more compressed\",\n",
    "            \"d) The sequence is encrypted\"\n",
    "        ],\n",
    "        \"answer\": \"c) The sequence becomes more compressed\",\n",
    "        \"explanation\": \"Replacing a pair of tokens with a new token in the BPE algorithm results in a more compressed sequence.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the benefit of using the BPE algorithm?\",\n",
    "        \"options\": [\n",
    "            \"a) It increases the storage requirements of the sequence\",\n",
    "            \"b) It decreases the processing time of the sequence\",\n",
    "            \"c) It allows for more efficient storage and processing of the sequence\",\n",
    "            \"d) It reduces the vocabulary size of the sequence\"\n",
    "        ],\n",
    "        \"answer\": \"c) It allows for more efficient storage and processing of the sequence\",\n",
    "        \"explanation\": \"The BPE algorithm allows for more efficient storage and processing of the sequence by compressing it into a variable amount of tokens.\"\n",
    "    }\n",
    "]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the primary goal of the BPE (Byte Pair Encoding) algorithm?',\n",
       "  'options': ['a) To encrypt byte sequences',\n",
       "   'b) To compress byte sequences into a variable amount of tokens',\n",
       "   'c) To decrypt byte sequences',\n",
       "   'd) To convert byte sequences into a fixed amount of tokens'],\n",
       "  'answer': 'b) To compress byte sequences into a variable amount of tokens',\n",
       "  'explanation': 'The BPE algorithm is used to compress byte sequences into a variable amount of tokens, allowing for more efficient storage and processing.'},\n",
       " {'question': 'How does the BPE algorithm find the pair of tokens to replace?',\n",
       "  'options': ['a) By randomly selecting pairs',\n",
       "   'b) By iterating through the sequence and finding the pair that occurs most frequently',\n",
       "   'c) By using a predefined set of rules',\n",
       "   'd) By using a machine learning model'],\n",
       "  'answer': 'b) By iterating through the sequence and finding the pair that occurs most frequently',\n",
       "  'explanation': 'The BPE algorithm iteratively finds the pair of tokens that occur most frequently in the sequence and replaces them with a new token.'},\n",
       " {'question': 'What happens when the BPE process is repeated?',\n",
       "  'options': ['a) The vocabulary size decreases',\n",
       "   'b) The vocabulary size remains the same',\n",
       "   'c) The vocabulary size increases',\n",
       "   'd) The sequence becomes less compressed'],\n",
       "  'answer': 'c) The vocabulary size increases',\n",
       "  'explanation': 'When the BPE process is repeated, the vocabulary size increases as more pairs of tokens are replaced with new tokens.'},\n",
       " {'question': 'What is the result of replacing a pair of tokens with a new token in the BPE algorithm?',\n",
       "  'options': ['a) The sequence becomes less compressed',\n",
       "   'b) The sequence remains the same',\n",
       "   'c) The sequence becomes more compressed',\n",
       "   'd) The sequence is encrypted'],\n",
       "  'answer': 'c) The sequence becomes more compressed',\n",
       "  'explanation': 'Replacing a pair of tokens with a new token in the BPE algorithm results in a more compressed sequence.'},\n",
       " {'question': 'What is the benefit of using the BPE algorithm?',\n",
       "  'options': ['a) It increases the storage requirements of the sequence',\n",
       "   'b) It decreases the processing time of the sequence',\n",
       "   'c) It allows for more efficient storage and processing of the sequence',\n",
       "   'd) It reduces the vocabulary size of the sequence'],\n",
       "  'answer': 'c) It allows for more efficient storage and processing of the sequence',\n",
       "  'explanation': 'The BPE algorithm allows for more efficient storage and processing of the sequence by compressing it into a variable amount of tokens.'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n```json\\n[\\n    {\\n        \"name\": \"a\",\\n        \"description\": \"bc\"\\n    }\\n]\\n```\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_str = \"\"\"\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"name\": \"a\",\n",
    "        \"description\": \"bc\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 2 column 1 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hashir\\anaconda3\\envs\\nvidia-dev\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\Hashir\\anaconda3\\envs\\nvidia-dev\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\Hashir\\anaconda3\\envs\\nvidia-dev\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 2 column 1 (char 1)"
     ]
    }
   ],
   "source": [
    "json.loads(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n```json\\n[\\n    {\\n        \"name\": \"a\",\\n        \"description\": \"bc\"\\n    }\\n]\\n```\\n',\n",
       " '\\n\\n[\\n    {\\n        \"name\": \"a\",\\n        \"description\": \"bc\"\\n    }\\n]\\n\\n')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "json_str2 = re.sub(r\"(```|json)\", \"\", json_str)\n",
    "\n",
    "json_str, json_str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'a', 'description': 'bc'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.loads(json_str2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvidia-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
