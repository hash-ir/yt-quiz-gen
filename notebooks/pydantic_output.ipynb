{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from pprint import pprint\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index.core.program import LLMTextCompletionProgram\n",
    "from pydantic import BaseModel, parse_obj_as\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import disk_offload\n",
    "\n",
    "from data.transcript import get_video_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.environ['GROQ_API_KEY']\n",
    "NVIDIA_API_KEY = os.environ['NVIDIA_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = NVIDIA(model=\"meta/llama-3.1-8b-instruct\", api_key=NVIDIA_API_KEY)\n",
    "llm = Groq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = get_video_transcript(url=\"https://www.youtube.com/watch?v=zduSFxRajkE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for item in transcript:\n",
    "    text += item['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hi everyone so in this video I'd like usto cover the process of tokenization inlarge language models now you see herethat I have a set face and that'sbecause uh tokenization is my leastfavorite part of working with largelanguage models but unfortunately it isnecessary to understand in some detailbecause it it is fairly hairy gnarly andthere's a lot of hidden foot guns to beaware of and a lot of oddness with largelanguage models typically traces back totokenization so what istokenization now in m\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topic(BaseModel):\n",
    "    name: str\n",
    "    description: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/Llama3-ChatQA-2-70B\")\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tokenizer.encode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sllm = llm.as_structured_llm(output_cls=Topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_extractor_prompt = f\"\"\"\n",
    "Given the transcript of a YouTube video tutorial:\n",
    "\n",
    "{text[:-100000]}\n",
    "\n",
    "Understand what this video is about and identify the main topics.\n",
    "Include as much description as possible for each topic.\n",
    "Make sure topics are not ambiguous and have meaningful segregation of content.\n",
    "\n",
    "Format your response as a JSON array of objects, where each object has two fields:\n",
    "- \"name\": The name of the topic (a short, concise title)\n",
    "- \"description\": A detailed description of the topic\n",
    "\n",
    "Example format:\n",
    "[\n",
    "  {{\n",
    "    \"name\": \"Topic 1\",\n",
    "    \"description\": \"Detailed description of Topic 1...\"\n",
    "  }},\n",
    "  {{\n",
    "    \"name\": \"Topic 2\",\n",
    "    \"description\": \"Detailed description of Topic 2...\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "Remove the preamble and ensure your output is valid JSON. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(topic_extractor_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Introduction to Tokenization', 'description': 'The video begins with an introduction to tokenization, a process essential for working with large language models. The speaker explains that tokenization is their least favorite part of working with large language models, but it is necessary to understand in detail due to its complexity and potential issues. The speaker also mentions that tokenization is the process of translating strings or text into sequences of tokens, which are the fundamental units of large language models.'}, {'name': 'Naive Tokenization', 'description': 'The speaker explains that in their previous video, they implemented a naive tokenization process, which was a character-level tokenizer. They loaded a training set, created a vocabulary of 65 possible characters, and created a lookup table for converting characters into tokens. The speaker then demonstrated how this process worked by tokenizing a string and encoding it into tokens.'}, {'name': 'BPE Encoding Algorithm', 'description': 'The speaker introduces the BPE (Byte Pair Encoding) algorithm, which is used to compress byte sequences into a variable amount of tokens. The algorithm iteratively finds the pair of tokens that occur most frequently and replaces them with a new token. The speaker explains that this process can be repeated to compress the sequence further and create a larger vocabulary.'}, {'name': 'Tokenization Issues', 'description': 'The speaker discusses various issues related to tokenization, including the complexity of tokenization, the potential for errors, and the impact on large language models. They explain that tokenization is at the heart of many issues, such as difficulties with spelling tasks, non-English languages, and simple arithmetic. The speaker also mentions that tokenization can be sensitive to whitespace, punctuation, and case, which can lead to inconsistent tokenization.'}, {'name': 'UTF-8 Encoding', 'description': 'The speaker explains that UTF-8 encoding is a variable-length encoding that can represent Unicode characters using between 1 to 4 bytes. They discuss the advantages of UTF-8 encoding, including its backwards compatibility with ASCII and its widespread use on the internet. However, the speaker notes that using raw UTF-8 bytes as tokens can lead to a vocabulary size of only 256 possible tokens, which is too small for most applications.'}, {'name': 'Tokenization in Large Language Models', 'description': 'The speaker discusses how tokenization is used in large language models, including the use of embedding tables and attention mechanisms. They explain that tokenization is essential for feeding text into language models, and that the choice of tokenization algorithm can have a significant impact on the performance of the model. The speaker also mentions that some researchers have proposed tokenization-free sequence modeling, but this is still an area of ongoing research.'}, {'name': 'Comparison of Tokenizers', 'description': 'The speaker compares the performance of different tokenizers, including the GPT-2 tokenizer and the GPT-4 tokenizer. They explain that the GPT-4 tokenizer has improved handling of whitespace and punctuation, which can lead to more efficient tokenization and better performance on certain tasks.'}, {'name': 'Implementation of Tokenization', 'description': 'The speaker begins to implement tokenization using the BPE algorithm, explaining that the goal is to take strings and feed them into language models by tokenizing them into integers in a fixed vocabulary. They discuss the challenges of supporting different languages and special characters, and explain that the Unicode standard is used to define a large vocabulary of characters.'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hashir\\AppData\\Local\\Temp\\ipykernel_2524\\482209594.py:3: PydanticDeprecatedSince20: `parse_obj_as` is deprecated. Use `pydantic.TypeAdapter.validate_python` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  topics = parse_obj_as(List[Topic], json_response)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    json_response = json.loads(response.text)\n",
    "    topics = parse_obj_as(List[Topic], json_response)\n",
    "    topics_dict = {topic.name: topic.description for topic in topics}\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Error: The LLM output was not valid JSON.\")\n",
    "except ValueError:\n",
    "    print(\"Error: The LLM output did not match the expected format.\")\n",
    "    \n",
    "topics = ', '.join(list(topics_dict.keys()))\n",
    "topics_out = f\"\"\"I have identified the following topics from the video: {topics}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have identified the following topics from the video: Introduction to Tokenization, Naive Tokenization, BPE Encoding Algorithm, Tokenization Issues, UTF-8 Encoding, Tokenization in Large Language Models, Comparison of Tokenizers, Implementation of Tokenization'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LLMStructuredPredictEndEvent\noutput\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=\"2 validation errors for ...antic.dev/2.9/v/missing\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m topics \u001b[38;5;241m=\u001b[39m \u001b[43msllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_extractor_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:265\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    258\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[0;32m    259\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[0;32m    263\u001b[0m )\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:431\u001b[0m, in \u001b[0;36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[0;32m    423\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[0;32m    424\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    428\u001b[0m     },\n\u001b[0;32m    429\u001b[0m )\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    433\u001b[0m     callback_manager\u001b[38;5;241m.\u001b[39mon_event_end(\n\u001b[0;32m    434\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[0;32m    435\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mEXCEPTION: e},\n\u001b[0;32m    436\u001b[0m         event_id\u001b[38;5;241m=\u001b[39mevent_id,\n\u001b[0;32m    437\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\llama_index\\core\\llms\\structured_llm.py:141\u001b[0m, in \u001b[0;36mStructuredLLM.complete\u001b[1;34m(self, prompt, formatted, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;129m@llm_completion_callback\u001b[39m()\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomplete\u001b[39m(\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, formatted: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    139\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompletionResponse:\n\u001b[0;32m    140\u001b[0m     complete_fn \u001b[38;5;241m=\u001b[39m chat_to_completion_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat)\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplete_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\llama_index\\core\\base\\llms\\generic_utils.py:173\u001b[0m, in \u001b[0;36mchat_to_completion_decorator.<locals>.wrapper\u001b[1;34m(prompt, **kwargs)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompletionResponse:\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# normalize input\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     messages \u001b[38;5;241m=\u001b[39m prompt_to_messages(prompt)\n\u001b[1;32m--> 173\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# normalize output\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_response_to_completion_response(chat_response)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:265\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    258\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[0;32m    259\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[0;32m    263\u001b[0m )\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:173\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[1;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[0;32m    165\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[0;32m    166\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m     },\n\u001b[0;32m    171\u001b[0m )\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    175\u001b[0m     callback_manager\u001b[38;5;241m.\u001b[39mon_event_end(\n\u001b[0;32m    176\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[0;32m    177\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mEXCEPTION: e},\n\u001b[0;32m    178\u001b[0m         event_id\u001b[38;5;241m=\u001b[39mevent_id,\n\u001b[0;32m    179\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\llama_index\\core\\llms\\structured_llm.py:109\u001b[0m, in \u001b[0;36mStructuredLLM.chat\u001b[1;34m(self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# NOTE: we are wrapping existing messages in a ChatPromptTemplate to\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# make this work with our FunctionCallingProgram, even though\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# the messages don't technically have any variables (they are already formatted)\u001b[39;00m\n\u001b[0;32m    107\u001b[0m chat_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate(message_templates\u001b[38;5;241m=\u001b[39m_escape_json(messages))\n\u001b[1;32m--> 109\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ChatResponse(\n\u001b[0;32m    113\u001b[0m     message\u001b[38;5;241m=\u001b[39mChatMessage(\n\u001b[0;32m    114\u001b[0m         role\u001b[38;5;241m=\u001b[39mMessageRole\u001b[38;5;241m.\u001b[39mASSISTANT, content\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mmodel_dump_json()\n\u001b[0;32m    115\u001b[0m     ),\n\u001b[0;32m    116\u001b[0m     raw\u001b[38;5;241m=\u001b[39moutput,\n\u001b[0;32m    117\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:265\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    258\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[0;32m    259\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[0;32m    263\u001b[0m )\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\llama_index\\llms\\openai\\base.py:963\u001b[0m, in \u001b[0;36mOpenAI.structured_predict\u001b[1;34m(self, llm_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m llm_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequired\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m llm_kwargs \u001b[38;5;28;01melse\u001b[39;00m llm_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    960\u001b[0m )\n\u001b[0;32m    961\u001b[0m \u001b[38;5;66;03m# by default structured prediction uses function calling to extract structured outputs\u001b[39;00m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;66;03m# here we force tool_choice to be required\u001b[39;00m\n\u001b[1;32m--> 963\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:265\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    258\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[0;32m    259\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[0;32m    263\u001b[0m )\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py:364\u001b[0m, in \u001b[0;36mLLM.structured_predict\u001b[1;34m(self, output_cls, prompt, llm_kwargs, **prompt_args)\u001b[0m\n\u001b[0;32m    356\u001b[0m program \u001b[38;5;241m=\u001b[39m get_program_for_llm(\n\u001b[0;32m    357\u001b[0m     output_cls,\n\u001b[0;32m    358\u001b[0m     prompt,\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    360\u001b[0m     pydantic_program_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_program_mode,\n\u001b[0;32m    361\u001b[0m )\n\u001b[0;32m    363\u001b[0m result \u001b[38;5;241m=\u001b[39m program(llm_kwargs\u001b[38;5;241m=\u001b[39mllm_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[1;32m--> 364\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\u001b[43mLLMStructuredPredictEndEvent\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nvidia-dev\\Lib\\site-packages\\pydantic\\main.py:209\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    208\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    211\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    215\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    216\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for LLMStructuredPredictEndEvent\noutput\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=\"2 validation errors for ...antic.dev/2.9/v/missing\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/model_type"
     ]
    }
   ],
   "source": [
    "topics = sllm.complete(topic_extractor_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 [conda:nvidia-dev]",
   "language": "python",
   "name": "nvidia-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
